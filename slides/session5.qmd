---
title: "HNU2000 - Session 5"
format:
  revealjs:
    embed-resources: true
    theme: default
---

## Notes collaboratives 

Lien vers le notepad : [https://pad.libreon.fr/s/TRJQIv-YZ#](https://pad.libreon.fr/s/TRJQIv-YZ#)


## Podcast sur la lecture d'aujourd'hui {.scrollable}

<!--
<audio controls>
  <source src="../media/Smits2023_selon_notebookLM.wav" type="audio/wav">Your browser does not support the audio element.
</audio> 
-->

<figure>
  <figcaption>Smits & Wevers (2023) vu par NotebookLM</figcaption>
  <audio controls src="https://github.com/alix-tz/hnu2000-a24/raw/refs/heads/main/media/Smits2023_selon_notebookLM_2.mp3"></audio>
  <a href="https://github.com/alix-tz/hnu2000-a24/raw/refs/heads/main/media/Smits2023_selon_notebookLM_2.mp3">Télécharger l'audio</a>
</figure>


### Retranscription :

> - Welcome to today's deep dive. We're really diving deep into a research paper all about using AI to analyze images. It's changing how we understand history and culture, which I gotta say, I find totally fascinating.
> - Yeah, it's a really cool area of research. So digital humanities focused heavily on text for a long time. I mean, it makes sense, right? Like that was the stuff that was easy to get digitally. Think Google Books, for example. That was huge for textual analysis. 
> - Oh, absolutely. Talk about a game changer. Suddenly, researchers had access to like this massive digital library. 
> - Exactly. But you're interested in images and you are so right to be. I mean, think about it. We're practically swimming in visuals these days. Photography, videos, you name it. And the really interesting part is that the historical record is full of them too. So the shift towards more and more visual stuff, it's made researchers realize that focusing on text alone, well, it was like we were missing half the story. 
> - Okay, I'm totally with you on that. But how does looking at like text and the images together actually change how we understand history? Couldn't we just look at a picture and get the same info? 
> - Well, not necessarily because context is everything. It's like, imagine you're looking at a historical photo of a busy city street, right? And then you see that same photo again, but this time it's got a caption that says, the calm before the storm. It hits different, doesn't it? 
> - Whoa, yeah, it totally does. You're right, it's like seeing the whole picture, not just one piece of the puzzle. 
> - Exactly, and that's the beauty of multimodality, understanding that like words and pictures, they work together to build meaning. It's not enough to just look at them separately. 
> - So we're talking about more than just the words on a page or a single image. It's about how those things interact to tell a story. 
> - Right, and you know what's interesting? This whole idea of multimodality, it's not actually new. Humans have always been multimodal. Think about it. We use gestures when we talk, our tone of voice changes, our facial expressions, we're constantly combining different modes of communication. It's just that in the past, when we, we were analyzing historical sources, we got really good at dissecting those things, separating them out. 
> - Okay. So then how do we put it all back together? How do we analyze text and images in a way that reflects how we actually experience the world? That's where AI comes in, right? 
> - You got it. And there's this really cool AI model that's making waves in the digital humanities world right now called CLIP. But before we get into that, maybe we should talk about how we even got here in the first place. 
> - I'm all ears. Walk me through it. 
> - Well, you see, traditionally, if you wanted to train AI to analyze images, you basically had to feed it a ton of images that were painstakingly labeled by humans. This is a cat. This is a dog. You get the idea. And as you can imagine, that gets really tedious and expensive, especially when you're dealing with potentially millions of historical images. 
> - Yeah. I can only imagine the eye strain. So how did researchers deal with that? There had to be a better way. Okay. So we were talking about the massive effort involved in training AI to analyze images. It sounded honestly kind of impossible. How do you even begin to teach a computer to see all the nuances in a historical photograph?
> - Right, it was a huge challenge for researchers. But, and this is a cool part, CLIP kind of swoops in and changes everything. 
> - CLIP, okay so, tell me more about this AI superstar.
> - So, CLIP stands for Contrastive Language Image Pretraining. And basically it learns in a totally different way: instead of needing all of those images labelled by hand, it anaylises massive datasets of image text pairs.
> - Image text pairs, so, are we talking about captions on old photos? Is that the kind of things?
> - Yeah, you got it. Like imagine you've got a museum archive, right? Thousands of photos, each with little descriptions. CLIP can actually learn from that and starts to connect the words from the descriptions to what it's seeing in the image.
> - So instead of needing someone to say: this is a cat, it could look at a picture with a caption like "Mrs Smiths and her beloved whiskers" and figure it out that way.
> - Exactly.


<!--
> - Okay, so get this, right? We're diving into visual history today, but not just like looking at old pictures. It's about how AI can help us unlock like secrets hidden within them. 
> - It's a whole new way of looking at the past, isn't it? 
> - Totally. This paper, a multimodal turn in digital humanities, really got me thinking. 
> - Yeah, it's a fascinating read. 
> - They're asking, how do we study visual trends across huge amounts of pictures? Like imagine trying to find patterns in a sea of faces. 
> - It's a huge challenge. Traditional methods that kind of fall short.
> - Right, because it's not like analyzing text where you've got words and sentences. 
> - Exactly, distant reading, that revolutionized text analysis, but images, whole different story. 
> - So how do we bridge that gap? Is this multimodal thing the key? 
> - Absolutely, it's about moving beyond just recognizing objects in a picture. 
> - Okay, so what, like understanding the meaning behind the image? 
> - Exactly, and connecting it to any text that goes with it. That's where multimodal models like CLIP come in. 
> - CLIP, all right, now you gotta tell me more about this CLIP thing.
> - So imagine you feed a computer program tons of images, right, and their descriptions. Billions of them. 
> - Okay, so it's learning what things look like and how they're described. 
> - Yes, and it starts to see these connections and it makes sense of images kind of like we do. 
> - So instead of just seeing, say, a family dinner. 
> - It gets the concept of family itself. 
> - Wow, even in different contexts. 
> - Exactly, and what's really cool, the paper talks about how CLIP can analyze images it wasn't specifically trained on. 
> - No way. 
> - Yeah, they used it to tell the difference between indoor and outdoor scenes. 
> - In like old photos. 
> - In magic lantern slides, actually. 
> - Oh, those old things. 
> - Yeah, and it's trickier than you'd think with the faded colors. Right. I'd probably get some of those wrong myself. 
> - Well, in this work it's interesting how we talk to CLIP matters. 
> - The prompts, they're key. 
> - Prompts, you mean like the questions we ask it. 
> - Exactly. Even small changes like outdoors indoors versus exterior interior can change how accurate CLIP is. 
> - Really? Just from that? 
> - Yeah, they have a table in the paper showing the differences. It's wild. 
> - Huh. So it's not just about having a ton of images but knowing how to, like, guide CLIP's interpretation. 
> - Precisely. And that raises a big question. How good is good enough when it comes to AI, right? 
> - Especially compared to, like, an actual historian analyzing things.
> - Exactly. There's a balance to strike between AI speed and the nuance of human experts. Right, because a human would pick up on details CLIP might miss, like the style of a building or the clothes people are wearing. 
> - You got it. And those details. Super important for understanding the context of an image.
> - Totally. 
> - Okay, this CLIP thing is blowing my mind already and we're just getting started. 

-->


<!-- apprentissage automatique -->



<!-- Notes de préparation 


Ressources pour préparation du cours:
- https://hal.science/hal-02379455
- https://sciencespo.hal.science/hal-02005537v1/file/pre-print-revancheneurones-reseaux.pdf



Quote :

> The Book was proving far more stubborn than Artemis had anticipated. It seemed to be almost actively resisting him. No matter which program he ran it through, the computer came up blank.
> 
> Artemis hardcopied every page, tacking them to the walls of his study. Sometimes it helped to have things on paper. The script was like nothing he’d seen before, and yet it was strangely familiar. Obviously a mixture of symbolic and character-based language, the text meandered around the page in no apparent order.
> 
> What the program needed was some frame of reference, some central point on which to build. He separated all the characters and ran comparisons with English, Chinese, Greek, Arabic, and with Cyrillic texts, even with Ogham. Nothing.
> 
> Moody with frustration, Artemis sent Juliet scurrying when she interrupted with sandwiches, and moved on to symbols. The most frequently recurring pictogram was a small male figure. Male, he presumed, though with the limited knowledge of the fairy anatomy he supposed it could be female. A thought struck him. Artemis opened the ancient languages file on his Power Translator and selected Egyptian.
> 
> At last. A hit. The male symbol was remarkably similar to the Anubis god representation on Tutankhamen’s inner-chamber hieroglyphics. This was consistent with his other findings. The first written human stories were about fairies, suggesting that their civilization predated man’s own. It would seem that the Egyptians had simply adapted an existing scripture to suit their needs.
> 
> There were other resemblances. But the characters were just dissimilar enough to slip through the computer’s net. This would have to be done manually. Each Gnommish figure had to be enlarged, printed, and then compared with the hieroglyphs.
> 
> Artemis felt the excitement of success thumping inside his rib cage. Almost every fairy pictogram or letter had an Egyptian counterpart. Most were universal, such as the sun or birds. But some seemed exclusively supernatural and had to be tailored to fit. The Anubis figure, for example, would make no sense as a dog god, so Artemis altered it to read king of the fairies.
> 
> By midnight, Artemis had successfully fed his findings into the Macintosh. All he had to do now was press Decode. He did so. What emerged was a long, intricate string of meaningless gibberish.
> 
> A normal child would have abandoned the task long since. The average adult would probably have been reduced to slapping the keyboard. But not Artemis. This book was testing him, and he would not allow it to win.
> 
> The letters were right, he was certain of it. It was just the order that was wrong. Rubbing the sleep from his eyes, Artemis glared at the pages again. Each segment was bordered by a solid line. This could represent paragraphs or chapters, but they were not meant to be read in the usual left to right, top to bottom fashion.
> 
> Artemis experimented. He tried the Arabic right to left and the Chinese columns. Nothing worked. Then he noticed that each page had one thing in common – a central section. The other pictograms were arranged around this pivotal area. So, a central starting point, perhaps. But where to go from there? Artemis scanned the pages for some other common factor. After several minutes he found it. There was on each page a tiny spearhead in the corner of one section. Could this be an arrow? A direction? Go this way? So the theory would be, start in the middle then follow the arrow. Reading in spirals.
> 
> The computer program wasn’t built to handle something like this, so Artemis had to improvise. With a craft knife and ruler, he dissected the first page of the Book and reassembled it in the traditional Western languages order – left to right, parallel rows. Then he rescanned the page and fed it through the modified Egyptian translator.
> 
> The computer hummed and whirred, converting all the information to binary. Several times it stopped to ask for confirmation of a character or symbol. This happened less and less as the machine learned the new language. Eventually two words flashed on the screen: File converted.
> 
> Fingers shaking from exhaustion and excitement, Artemis clicked Print. A single page scrolled from the LaserWriter. It was in English now. Yes there were mistakes, some fine-tuning needed, but it was perfectly legible, and, more importantly, perfectly understandable.

-->